import cv2
import numpy as np
import onnxruntime as ort
import subprocess

class YOLOv8:
    def __init__(self, model_path, conf_thres=0.2, iou_thres=0.5):
        self.conf_threshold = conf_thres
        self.iou_threshold = iou_thres

        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        self.session = ort.InferenceSession(model_path, so, providers=['CPUExecutionProvider'])

        self.get_input_details()
        self.get_output_details()

    def get_input_details(self):
        model_inputs = self.session.get_inputs()
        self.input_names = [model_inputs[i].name for i in range(len(model_inputs))]
        self.input_shape = model_inputs[0].shape
        self.input_height = self.input_shape[2]
        self.input_width = self.input_shape[3]

    def get_output_details(self):
        model_outputs = self.session.get_outputs()
        self.output_names = [model_outputs[i].name for i in range(len(model_outputs))]

    def preprocess(self, image):
        h, w = image.shape[:2]
        scale = min(self.input_height / h, self.input_width / w)
        new_h, new_w = int(h * scale), int(w * scale)
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

        canvas = np.full((self.input_height, self.input_width, 3), 114, dtype=np.uint8)
        top = (self.input_height - new_h) // 2
        left = (self.input_width - new_w) // 2
        canvas[top:top+new_h, left:left+new_w] = resized

        image_data = canvas.astype(np.float32) / 255.0
        image_data = np.transpose(image_data, (2, 0, 1))
        image_data = np.expand_dims(image_data, axis=0)
        return image_data, (h, w), (top, left), scale

    def detect(self, image):
        input_image, original_shape, padding, scale = self.preprocess(image)
        outputs = self.session.run(self.output_names, {self.input_names[0]: input_image})
        predictions = np.squeeze(outputs[0]).T
        scores = np.max(predictions[:, 4:], axis=1)
        predictions = predictions[scores > self.conf_threshold, :]
        scores = scores[scores > self.conf_threshold]

        if len(scores) == 0:
            return [], [], []

        class_ids = np.argmax(predictions[:, 4:], axis=1)
        boxes = self.extract_boxes(predictions, original_shape, padding, scale)
        indices = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), self.conf_threshold, self.iou_threshold)

        if len(indices) > 0:
            indices = indices.flatten()
            return boxes[indices], scores[indices], class_ids[indices]

        return [], [], []

    def extract_boxes(self, predictions, original_shape, padding, scale):
        boxes = predictions[:, :4]
        h, w = original_shape
        pad_top, pad_left = padding

        boxes[:, [0, 2]] = (boxes[:, [0, 2]] - pad_left) / scale
        boxes[:, [1, 3]] = (boxes[:, [1, 3]] - pad_top) / scale

        boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2
        boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2
        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]
        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]

        boxes[:, 0] = np.clip(boxes[:, 0], 0, w)
        boxes[:, 1] = np.clip(boxes[:, 1], 0, h)
        boxes[:, 2] = np.clip(boxes[:, 2], 0, w)
        boxes[:, 3] = np.clip(boxes[:, 3], 0, h)
        return boxes

# --- Classes ---
CLASSES = ['Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask',
           'NO-Safety Vest', 'Person', 'Safety Cone',
           'Safety Vest', 'machinery', 'vehicle']
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

def video_stream():
    model = YOLOv8("best.onnx", conf_thres=0.5)

    width, height = 320, 240
    cmd = [
        "libcamera-vid",
        "--inline", "-t", "0",
        "--width", str(width), "--height", str(height),
        "--codec", "yuv420",
        "-o", "-"
    ]

    pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, bufsize=width*height*3)

    while True:
        yuv_size = width * height + (width * height) // 2
        raw_data = pipe.stdout.read(yuv_size)

        if not raw_data:
            print("‚ùå No data from camera.")
            break

        # Convert YUV420 to BGR
        yuv = np.frombuffer(raw_data, dtype=np.uint8).reshape((height * 3 // 2, width))
        bgr_frame = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR_I420)

        boxes, scores, class_ids = model.detect(bgr_frame)

        for box, score, class_id in zip(boxes, scores, class_ids):
            x1, y1, x2, y2 = box.astype(int)
            label = f"{CLASSES[class_id]}: {score:.2f}"
            color = COLORS[class_id]
            cv2.rectangle(bgr_frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(bgr_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        cv2.imshow("PPE Detection - Pi Camera", bgr_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    pipe.terminate()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    video_stream()
